{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52e7dd2f-a5eb-4e89-8877-61620f3b110f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "import stanza\n",
    "from typing import List, Set, Dict\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28b8ae75-6599-4cad-946b-23cbde117556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LOADING FASTTEXT MODEL\n",
      "================================================================================\n",
      "This may take 10 minutes for a 7GB file...\n",
      "\n",
      "Loading FastText model with subword information...\n",
      "Model loaded successfully!\n",
      "Vector dimension: 300\n",
      "Model has subword information - can handle any word!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PART 1: LOAD FASTTEXT MODEL\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"LOADING FASTTEXT MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load the .bin file\n",
    "bin_file_path = '/Users/theodorselimovic/Sciences Po/Material/word vectors/cc.sv.300.bin'\n",
    "\n",
    "print(\"Loading FastText model with subword information...\")\n",
    "model = fasttext.load_model(bin_file_path)\n",
    "print(f\"Model loaded successfully!\")\n",
    "print(f\"Vector dimension: {model.get_dimension()}\")\n",
    "print(f\"Model has subword information - can handle any word!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "458e68e8-9ff8-415d-ac1d-f383c5ce2c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FINDING SIMILAR WORDS TO SEED TERMS\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "Processing category: RESILIENCE\n",
      "Seed words: resiliens, motståndskraft, återhämtning\n",
      "============================================================\n",
      "Processing 'resiliens'...\n",
      "  Found 50 similar words\n",
      "Processing 'motståndskraft'...\n",
      "  Found 50 similar words\n",
      "Processing 'återhämtning'...\n",
      "  Found 50 similar words\n",
      "\n",
      "Top 10 similar words for resilience:\n",
      "  Resiliens                 0.7995\n",
      "  återhämtningen            0.7994\n",
      "  motståndskraften          0.7752\n",
      "  återhämtningsprocess      0.7690\n",
      "  resiliensen               0.7678\n",
      "  återhämtnings             0.7638\n",
      "  återhämtningstid          0.7624\n",
      "  återhämtningsperiod       0.7614\n",
      "  återhämtningar            0.7378\n",
      "  tålighet                  0.7364\n",
      "\n",
      "============================================================\n",
      "Processing category: RISK\n",
      "Seed words: risk, riskbedömning, sårbarhet\n",
      "============================================================\n",
      "Processing 'risk'...\n",
      "  Found 50 similar words\n",
      "Processing 'riskbedömning'...\n",
      "  Found 50 similar words\n",
      "Processing 'sårbarhet'...\n",
      "  Found 50 similar words\n",
      "\n",
      "Top 10 similar words for risk:\n",
      "  riskbedömningen           0.8459\n",
      "  riskbedömningar           0.8196\n",
      "  risken                    0.8148\n",
      "  sårbarheten               0.7989\n",
      "  riskbedömningarna         0.7899\n",
      "  riskbedömningsmetod       0.7790\n",
      "  miljöriskbedömning        0.7545\n",
      "  sårbarheter               0.7482\n",
      "  Riskbedömning             0.7457\n",
      "  fallriskbedömning         0.7377\n",
      "\n",
      "============================================================\n",
      "Processing category: KOMPLEXITY\n",
      "Seed words: komplex, svår, komplicerad, beroende, utmaning, otydlig\n",
      "============================================================\n",
      "Processing 'komplex'...\n",
      "  Found 50 similar words\n",
      "Processing 'svår'...\n",
      "  Found 50 similar words\n",
      "Processing 'komplicerad'...\n",
      "  Found 50 similar words\n",
      "Processing 'beroende'...\n",
      "  Found 50 similar words\n",
      "Processing 'utmaning'...\n",
      "  Found 50 similar words\n",
      "Processing 'otydlig'...\n",
      "  Found 50 similar words\n",
      "\n",
      "Top 10 similar words for komplexity:\n",
      "  utmaningen                0.8567\n",
      "  jätteutmaning             0.8111\n",
      "  jättesvår                 0.8105\n",
      "  utmaningar                0.8036\n",
      "  utmaningarna              0.7817\n",
      "  invecklad                 0.7794\n",
      "  teknikutmaning            0.7748\n",
      "  -utmaning                 0.7734\n",
      "  omöjlig                   0.7733\n",
      "  komplex                   0.7727\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PART 2: DEFINE SEED TERMS AND FIND SIMILAR WORDS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"FINDING SIMILAR WORDS TO SEED TERMS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Define your seed terms\n",
    "seed_terms = {\n",
    "    'resilience': ['resiliens', 'motståndskraft', 'återhämtning'],\n",
    "    'risk': ['risk', 'riskanalys', 'riskbedömning', 'sårbarhet'],\n",
    "    'komplexity': ['komplex', 'svår', 'komplicerad', 'utmaning', 'otydlig'],\n",
    "    'dependencies': ['beroende', 'ömsesidighet'],\n",
    "    'actors': ['kommun', 'stat', 'länsstyrelse', 'region', 'näringsliv', 'civilsamhälle', 'förening']\n",
    "    # Add more categories as needed\n",
    "}\n",
    "\n",
    "def find_similar_words(model, seed_words: List[str], top_n: int = 50) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Find words similar to a list of seed words using FastText.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : fasttext.FastText._FastText\n",
    "        The loaded FastText model\n",
    "    seed_words : List[str]\n",
    "        List of seed words to find similar words for\n",
    "    top_n : int\n",
    "        Number of similar words to retrieve per seed word\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Dict mapping words to their maximum similarity scores across all seed words\n",
    "    \"\"\"\n",
    "    similar_words = {}\n",
    "    \n",
    "    for seed_word in seed_words:\n",
    "        print(f\"Processing '{seed_word}'...\")\n",
    "        \n",
    "        try:\n",
    "            # Get nearest neighbors\n",
    "            # Returns: [(word, similarity), ...]\n",
    "            neighbors = model.get_nearest_neighbors(seed_word, k=top_n)\n",
    "            \n",
    "            for similarity, word in neighbors:\n",
    "                if word in similar_words:\n",
    "                    # Keep maximum similarity score if word appears multiple times\n",
    "                    similar_words[word] = max(similar_words[word], similarity)\n",
    "                else:\n",
    "                    similar_words[word] = similarity\n",
    "                    \n",
    "            print(f\"  Found {len(neighbors)} similar words\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error processing '{seed_word}': {e}\")\n",
    "    \n",
    "    return similar_words\n",
    "\n",
    "# Find similar words for each category\n",
    "all_similar_words = {}\n",
    "\n",
    "for category, seeds in seed_terms.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing category: {category.upper()}\")\n",
    "    print(f\"Seed words: {', '.join(seeds)}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    similar = find_similar_words(model, seeds, top_n=50)\n",
    "    all_similar_words[category] = similar\n",
    "    \n",
    "    # Show top 10 most similar words\n",
    "    sorted_similar = sorted(similar.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    print(f\"\\nTop 10 similar words for {category}:\")\n",
    "    for word, score in sorted_similar:\n",
    "        print(f\"  {word:25s} {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65de58f9-e15b-4eb0-b76f-9b63d4122e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PART 3: CREATE EXPANDED TERM LIST\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CREATING EXPANDED TERM LISTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Combine all similar words with their categories\n",
    "expanded_terms = []\n",
    "\n",
    "for category, similar_words in all_similar_words.items():\n",
    "    for word, similarity in similar_words.items():\n",
    "        expanded_terms.append({\n",
    "            'category': category,\n",
    "            'word': word,\n",
    "            'similarity_score': similarity\n",
    "        })\n",
    "\n",
    "df_expanded = pd.DataFrame(expanded_terms)\n",
    "\n",
    "# Sort by similarity score\n",
    "df_expanded = df_expanded.sort_values(['category', 'similarity_score'], \n",
    "                                      ascending=[True, False])\n",
    "\n",
    "print(f\"\\nTotal expanded terms: {len(df_expanded)}\")\n",
    "print(f\"Terms by category:\")\n",
    "print(df_expanded.groupby('category').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5694e4-5709-4d09-b3a7-5268b72e36ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PART 4: LEMMATIZE EXPANDED TERMS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LEMMATIZING EXPANDED TERMS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Initialize Stanza\n",
    "print(\"Loading Stanza Swedish model...\")\n",
    "nlp = stanza.Pipeline('sv', processors='tokenize,pos,lemma', use_gpu=False)\n",
    "print(\"Model loaded!\\n\")\n",
    "\n",
    "def lemmatize_word(word: str, nlp) -> str:\n",
    "    \"\"\"\n",
    "    Lemmatize a single word using Stanza.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    word : str\n",
    "        Word to lemmatize\n",
    "    nlp : stanza.Pipeline\n",
    "        Stanza pipeline\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    str : Lemmatized word\n",
    "    \"\"\"\n",
    "    try:\n",
    "        doc = nlp(word)\n",
    "        if doc.sentences and doc.sentences[0].words:\n",
    "            return doc.sentences[0].words[0].lemma.lower()\n",
    "        return word.lower()\n",
    "    except:\n",
    "        return word.lower()\n",
    "\n",
    "print(\"Lemmatizing expanded terms...\")\n",
    "df_expanded['lemma'] = df_expanded['word'].apply(lambda x: lemmatize_word(x, nlp))\n",
    "\n",
    "# Remove duplicates after lemmatization\n",
    "print(f\"Terms before deduplication: {len(df_expanded)}\")\n",
    "df_expanded = df_expanded.drop_duplicates(subset=['category', 'lemma'])\n",
    "print(f\"Terms after deduplication: {len(df_expanded)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bab9c9-a99b-4178-894f-7ec5e374d5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PART 5: SAVE RESULTS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for category in df_expanded['category'].unique():\n",
    "    cat_data = df_expanded[df_expanded['category'] == category]\n",
    "    print(f\"\\n{category.upper()}: {len(cat_data)} unique lemmas\")\n",
    "    print(\"Top 15 terms by similarity:\")\n",
    "    top_terms = cat_data.nlargest(15, 'similarity_score')\n",
    "    for _, row in top_terms.iterrows():\n",
    "        print(f\"  {row['lemma']:25s} (original: {row['word']:25s}) {row['similarity_score']:.4f}\")\n",
    "\n",
    "# Save to CSV\n",
    "output_file = 'expanded_terms_lemmatized.csv'\n",
    "df_expanded.to_csv(output_file, index=False, encoding='utf-8')\n",
    "print(f\"\\n\\nSaved expanded terms to: {output_file}\")\n",
    "\n",
    "# Create a simple list of lemmas by category for easy filtering\n",
    "category_lemmas = {}\n",
    "for category in df_expanded['category'].unique():\n",
    "    category_lemmas[category] = set(\n",
    "        df_expanded[df_expanded['category'] == category]['lemma'].tolist()\n",
    "    )\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"READY FOR ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(\"You can now use the lemmatized terms to filter your sentence dataframe.\")\n",
    "print(\"\\nExample usage:\")\n",
    "print(\"  resilience_lemmas = category_lemmas['resilience']\")\n",
    "print(\"  df_resilience = df_sentences[\")\n",
    "print(\"      df_sentences['sentence_text'].apply(\")\n",
    "print(\"          lambda x: any(lemma in x.split() for lemma in resilience_lemmas)\")\n",
    "print(\"      )\")\n",
    "print(\"  ]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965ac4c3-2a34-4432-96b6-44b7a38194ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# OPTIONAL: MEMORY CLEANUP\n",
    "# ============================================================================\n",
    "\n",
    "# If you're done with the model and need to free memory:\n",
    "del model\n",
    "gc.collect()\n",
    "print(\"\\nFreed memory by deleting model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
