{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30d5a290-eb43-4f11-978d-b7d87930bee2",
   "metadata": {},
   "source": [
    "# This code does data wrangling of the corpus\n",
    "The code takes the unprocessed corpus that was created through reading the PDFs. It cleans up the corpus through the usual processes (lemmatization, removal of filler words). Importantly, it does not tokenize on the word level, but keeps the sentence structure to provide more insight into contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f94550-f38b-496b-857d-7a6297917413",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyreadr\n",
    "import stanza\n",
    "import re\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1743099-d64b-401f-9a40-9a6dc1a5a662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Swedish language model (run this once, then comment out)\n",
    "# stanza.download('sv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbdd9de-59a7-4e17-a49e-d005c1f44bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Swedish stopwords\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords', quiet=True)\n",
    "swedish_stopwords = set(stopwords.words('swedish'))\n",
    "\n",
    "# Your custom stopwords (artifacts and missed words)\n",
    "custom_stopwords = {\n",
    "    \"samt\",\n",
    "    \"│\",\n",
    "    \"_____________________________________________________________________________\",\n",
    "    \"▪\",\n",
    "    \"underbilaga\",\n",
    "    \"rsa\",\n",
    "    \"2023-2026\"\n",
    "}\n",
    "\n",
    "# Combine all stopwords\n",
    "all_stopwords = swedish_stopwords.union(custom_stopwords)\n",
    "\n",
    "print(f\"Using {len(all_stopwords)} stopwords ({len(custom_stopwords)} custom)\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45b862e-eabd-4aee-a287-32817485401c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your RDS file\n",
    "print(\"Loading RDS file...\")\n",
    "rds_data = pyreadr.read_r('/Users/theodorselimovic/Library/CloudStorage/OneDrive-Personal/Sciences Po/Master Thesis/Text analysis code/R project/readtext_success.rds')\n",
    "df = rds_data[list(rds_data.keys())[0]]\n",
    "\n",
    "print(f\"Loaded dataframe with {len(df)} rows and {len(df.columns)} columns\")\n",
    "print(f\"Columns: {df.columns.tolist()}\\n\")\n",
    "\n",
    "# Use 'file' column for document identification\n",
    "if 'file' not in df.columns:\n",
    "    raise ValueError(\"Column 'file' not found. Available: \" + str(df.columns.tolist()))\n",
    "\n",
    "doc_id_col = 'file'\n",
    "print(f\"Using '{doc_id_col}' column for document identification\\n\")\n",
    "\n",
    "def parse_document_name(doc_name: str) -> Dict[str, str]:\n",
    "    \"\"\"Parse document name to extract municipality, year, and maskad status.\"\"\"\n",
    "    doc_name = doc_name.replace('.pdf', '')\n",
    "    parts = doc_name.split()\n",
    "    \n",
    "    municipality = parts[1] if len(parts) > 1 else None\n",
    "    year = parts[2] if len(parts) > 2 else None\n",
    "    maskad = \"Maskad\" in doc_name\n",
    "    \n",
    "    return {\n",
    "        'municipality': municipality,\n",
    "        'year': year,\n",
    "        'maskad': maskad\n",
    "    }\n",
    "\n",
    "def lemmatize_sentence(sentence, remove_stopwords=True):\n",
    "    \"\"\"Lemmatize a single sentence and return lemmatized text and word count.\"\"\"\n",
    "    lemmatized_words = []\n",
    "    \n",
    "    for word in sentence.words:\n",
    "        lemma = word.lemma.lower()\n",
    "        original = word.text\n",
    "        \n",
    "        # Skip punctuation-only tokens\n",
    "        if re.match(r'^[^\\w\\s]+$', original):\n",
    "            continue\n",
    "        \n",
    "        # Check if it's a stopword\n",
    "        if remove_stopwords:\n",
    "            if lemma in all_stopwords or original.lower() in all_stopwords:\n",
    "                continue\n",
    "        \n",
    "        lemmatized_words.append(lemma)\n",
    "    \n",
    "    lemmatized_text = ' '.join(lemmatized_words)\n",
    "    word_count = len(lemmatized_words)\n",
    "    \n",
    "    return lemmatized_text, word_count\n",
    "\n",
    "def process_document_to_sentences(doc_name, text, remove_stopwords=True):\n",
    "    \"\"\"Process a document and return a list of sentence-level records.\"\"\"\n",
    "    if pd.isna(text) or text == '':\n",
    "        return []\n",
    "    \n",
    "    metadata = parse_document_name(doc_name)\n",
    "    doc = nlp(text)\n",
    "    sentences_data = []\n",
    "    \n",
    "    for sent_idx, sentence in enumerate(doc.sentences, 1):\n",
    "        lemmatized_text, word_count = lemmatize_sentence(sentence, remove_stopwords)\n",
    "        \n",
    "        if lemmatized_text.strip():\n",
    "            sentence_record = {\n",
    "                'doc_id': doc_name,\n",
    "                'municipality': metadata['municipality'],\n",
    "                'year': metadata['year'],\n",
    "                'maskad': metadata['maskad'],\n",
    "                'sentence_id': sent_idx,\n",
    "                'sentence_text': lemmatized_text,\n",
    "                'word_count': word_count\n",
    "            }\n",
    "            sentences_data.append(sentence_record)\n",
    "    \n",
    "    return sentences_data\n",
    "\n",
    "# Initialize Swedish pipeline with lemmatization (WITHOUT mwt)\n",
    "print(\"Loading Stanza Swedish model...\")\n",
    "nlp = stanza.Pipeline('sv', processors='tokenize,pos,lemma', use_gpu=False)\n",
    "print(\"Model loaded successfully!\\n\")\n",
    "\n",
    "# Process all documents into sentence-level data\n",
    "print(\"Processing documents to sentence-level data...\")\n",
    "print(\"(This may take a while for large corpora)\\n\")\n",
    "\n",
    "all_sentences = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    print(f\"Processing document {idx + 1}/{len(df)}...\", end='\\r')\n",
    "    \n",
    "    doc_id = row[doc_id_col]\n",
    "    text = row['text']\n",
    "    \n",
    "    sentences = process_document_to_sentences(doc_id, text, remove_stopwords=True)\n",
    "    all_sentences.extend(sentences)\n",
    "\n",
    "print(f\"\\nProcessing complete!                                \")\n",
    "\n",
    "# Create sentence-level dataframe\n",
    "df_sentences = pd.DataFrame(all_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a165778b-22be-40af-a3e0-b3b2fa1e5b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display statistics\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"STATISTICS\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Original documents: {len(df)}\")\n",
    "print(f\"Total sentences extracted: {len(df_sentences)}\")\n",
    "print(f\"Average sentences per document: {len(df_sentences) / len(df):.1f}\")\n",
    "print(f\"Average words per sentence: {df_sentences['word_count'].mean():.1f}\")\n",
    "print(f\"Median words per sentence: {df_sentences['word_count'].median():.0f}\")\n",
    "print(f\"Min words per sentence: {df_sentences['word_count'].min()}\")\n",
    "print(f\"Max words per sentence: {df_sentences['word_count'].max()}\")\n",
    "\n",
    "# Show word count distribution\n",
    "print(f\"\\nWord count distribution:\")\n",
    "print(df_sentences['word_count'].describe())\n",
    "\n",
    "# Show sentence length quartiles\n",
    "print(f\"\\nSentence length quartiles:\")\n",
    "quartiles = df_sentences['word_count'].quantile([0.25, 0.5, 0.75, 0.95])\n",
    "print(f\"25th percentile: {quartiles[0.25]:.0f} words\")\n",
    "print(f\"50th percentile: {quartiles[0.50]:.0f} words\")\n",
    "print(f\"75th percentile: {quartiles[0.75]:.0f} words\")\n",
    "print(f\"95th percentile: {quartiles[0.95]:.0f} words\")\n",
    "\n",
    "# Show example sentences\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"EXAMPLE SENTENCES\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "import random\n",
    "\n",
    "random_id = random.choice(df_sentences['doc_id'].unique())\n",
    "subset = df_sentences[df_sentences['doc_id'] == random_id]\n",
    "\n",
    "for idx in range(min(15, len(subset))):\n",
    "    row = subset.iloc[idx]\n",
    "    print(f\"\\nDocument: {row['doc_id']}\")\n",
    "    print(f\"Municipality: {row['municipality']}, Year: {row['year']}, Maskad: {row['maskad']}\")\n",
    "    print(f\"Sentence {row['sentence_id']} ({row['word_count']} words):\")\n",
    "    print(f\"Lemmatized: {row['sentence_text'][:500]}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27c6843-a1f6-4e85-a462-aae1dc033015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Save to Parquet\n",
    "print(f\"\\n{'='*80}\")\n",
    "df_sentences.to_parquet('sentences_lemmatized.parquet', index=False)\n",
    "print(\"Saved to: sentences_lemmatized.parquet\")\n",
    "print(\"\\nYour sentence-level data is ready for word vector analysis!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
