{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c478932-8465-47db-bb38-fcca387cfd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "OCR Processing Script for Swedish RSA PDF Documents.\n",
    "\n",
    "This module processes PDF files that failed regular text extraction (e.g., via\n",
    "readtext in R). It uses Tesseract OCR with Swedish language support to extract\n",
    "text from scanned or image-based PDFs.\n",
    "\n",
    "The script is designed for the Swedish Risk Analysis Text-as-Data Project,\n",
    "processing municipal RSA (Risk and Vulnerability Analysis) reports.\n",
    "\n",
    "IMPORTANT:\n",
    "    - Swedish language support in Tesseract is REQUIRED (no English fallback)\n",
    "    - All outputs are in Parquet format for Python workflows\n",
    "    - Designed to work in Jupyter Lab (uses tqdm.auto, proper logging)\n",
    "\n",
    "Output Format:\n",
    "    The primary output (ocr_readtext_format.parquet) matches the readtext R \n",
    "    package structure:\n",
    "    - file: PDF filename (e.g., \"RSA Ale 2015 Maskad.pdf\")\n",
    "    - text: Extracted text content\n",
    "\n",
    "    This enables direct concatenation with converted readtext output before\n",
    "    Stanza NLP preprocessing.\n",
    "\n",
    "Example Usage (Command Line):\n",
    "    # Process all PDFs in a directory\n",
    "    python ocr_swedish_pdfs.py -i ./pdfs -o ./output\n",
    "\n",
    "    # Process specific files from a list\n",
    "    python ocr_swedish_pdfs.py -i ./pdfs -o ./output -f failed_files.txt\n",
    "\n",
    "Example Usage (Jupyter Lab):\n",
    "    from pathlib import Path\n",
    "    from ocr_swedish_pdfs_improved import run_pipeline, ProcessingConfig\n",
    "    \n",
    "    config = ProcessingConfig(\n",
    "        language=\"swe+eng\",\n",
    "        dpi=300,\n",
    "        workers=1,  # Recommended for Jupyter\n",
    "    )\n",
    "    \n",
    "    results, summary = run_pipeline(\n",
    "        input_dir=Path(\"./pdfs\"),\n",
    "        output_dir=Path(\"./output\"),\n",
    "        config=config,\n",
    "    )\n",
    "\n",
    "Requirements:\n",
    "    Python packages: pytesseract, pdf2image, pillow, pandas, tqdm, pyarrow\n",
    "    System: Tesseract OCR with Swedish language pack (tesseract-ocr-swe), Poppler\n",
    "\n",
    "Author: Swedish Risk Analysis Project\n",
    "License: MIT\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import re\n",
    "import sys\n",
    "import tempfile\n",
    "from abc import ABC, abstractmethod\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from datetime import datetime\n",
    "from enum import Enum, auto\n",
    "from pathlib import Path\n",
    "from typing import Iterator, Sequence\n",
    "\n",
    "# =============================================================================\n",
    "# Constants and Configuration\n",
    "# =============================================================================\n",
    "\n",
    "# Default processing parameters\n",
    "DEFAULT_DPI = 300\n",
    "DEFAULT_LANGUAGE = \"swe+eng\"  # Swedish primary, English for mixed content\n",
    "DEFAULT_MIN_TEXT_LENGTH = 500\n",
    "DEFAULT_WORKERS = 1\n",
    "\n",
    "# Jupyter Lab detection\n",
    "IS_JUPYTER = False\n",
    "try:\n",
    "    from IPython import get_ipython\n",
    "    if get_ipython() is not None:\n",
    "        IS_JUPYTER = True\n",
    "except (ImportError, NameError):\n",
    "    pass\n",
    "\n",
    "# Tesseract configuration\n",
    "# PSM 1: Automatic page segmentation with OSD (Orientation and Script Detection)\n",
    "# OEM 3: Default, based on what is available (LSTM + Legacy)\n",
    "TESSERACT_CONFIG = \"--psm 1 --oem 3\"\n",
    "\n",
    "# File patterns\n",
    "PDF_EXTENSIONS = {\".pdf\", \".PDF\"}\n",
    "\n",
    "# RSA filename parsing pattern\n",
    "# Matches: RSA [Municipality] [Year] [Maskad].pdf\n",
    "RSA_FILENAME_PATTERN = re.compile(\n",
    "    r\"^RSA\\s+(?P<municipality>.+?)\\s+(?P<year>(?:19|20)\\d{2})(?:\\s+(?P<maskad>[Mm]askad))?\\s*\\.pdf$\",\n",
    "    re.IGNORECASE,\n",
    ")\n",
    "\n",
    "# Fallback year pattern for non-standard filenames\n",
    "YEAR_PATTERN = re.compile(r\"\\b(19|20)\\d{2}\\b\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Enums and Data Classes\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "class ProcessingStatus(Enum):\n",
    "    \"\"\"Status codes for PDF processing results.\"\"\"\n",
    "\n",
    "    SUCCESS = auto()\n",
    "    FAILED_SHORT = auto()\n",
    "    FAILED_OCR = auto()\n",
    "    FAILED_CONVERSION = auto()\n",
    "    FAILED_IO = auto()\n",
    "    SKIPPED = auto()\n",
    "\n",
    "    def is_success(self) -> bool:\n",
    "        \"\"\"Check if status indicates successful extraction.\"\"\"\n",
    "        return self == ProcessingStatus.SUCCESS\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DocumentMetadata:\n",
    "    \"\"\"Metadata extracted from RSA document filename.\n",
    "\n",
    "    Attributes:\n",
    "        filename: Original PDF filename\n",
    "        municipality: Swedish municipality name\n",
    "        year: Publication year (4-digit string or 'unknown')\n",
    "        is_masked: Whether document is marked as 'Maskad' (redacted)\n",
    "    \"\"\"\n",
    "\n",
    "    filename: str\n",
    "    municipality: str = \"unknown\"\n",
    "    year: str = \"unknown\"\n",
    "    is_masked: bool = False\n",
    "\n",
    "    @classmethod\n",
    "    def from_filename(cls, filename: str) -> DocumentMetadata:\n",
    "        \"\"\"Parse metadata from RSA document filename.\n",
    "\n",
    "        Expected format: RSA [municipality] [year] [Maskad].pdf\n",
    "\n",
    "        Args:\n",
    "            filename: PDF filename to parse\n",
    "\n",
    "        Returns:\n",
    "            DocumentMetadata instance with extracted fields\n",
    "\n",
    "        Examples:\n",
    "            >>> DocumentMetadata.from_filename(\"RSA Ale 2015 Maskad.pdf\")\n",
    "            DocumentMetadata(filename='RSA Ale 2015 Maskad.pdf',\n",
    "                           municipality='Ale', year='2015', is_masked=True)\n",
    "            >>> DocumentMetadata.from_filename(\"RSA Gnosjö 2019.pdf\")\n",
    "            DocumentMetadata(filename='RSA Gnosjö 2019.pdf',\n",
    "                           municipality='Gnosjö', year='2019', is_masked=False)\n",
    "        \"\"\"\n",
    "        # Try structured pattern first\n",
    "        match = RSA_FILENAME_PATTERN.match(filename)\n",
    "        if match:\n",
    "            return cls(\n",
    "                filename=filename,\n",
    "                municipality=match.group(\"municipality\").strip(),\n",
    "                year=match.group(\"year\"),\n",
    "                is_masked=match.group(\"maskad\") is not None,\n",
    "            )\n",
    "\n",
    "        # Fallback: extract what we can\n",
    "        name_without_ext = Path(filename).stem\n",
    "        is_masked = \"maskad\" in name_without_ext.lower()\n",
    "\n",
    "        # Remove 'RSA' prefix and 'Maskad' suffix\n",
    "        clean_name = re.sub(r\"^RSA\\s*\", \"\", name_without_ext, flags=re.IGNORECASE)\n",
    "        clean_name = re.sub(r\"\\s*[Mm]askad\\s*$\", \"\", clean_name)\n",
    "\n",
    "        # Extract year\n",
    "        year_match = YEAR_PATTERN.search(clean_name)\n",
    "        year = year_match.group(0) if year_match else \"unknown\"\n",
    "\n",
    "        # Remaining text is municipality\n",
    "        municipality = YEAR_PATTERN.sub(\"\", clean_name).strip()\n",
    "        municipality = municipality if municipality else \"unknown\"\n",
    "\n",
    "        return cls(\n",
    "            filename=filename,\n",
    "            municipality=municipality,\n",
    "            year=year,\n",
    "            is_masked=is_masked,\n",
    "        )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class OCRResult:\n",
    "    \"\"\"Result of OCR processing for a single document.\n",
    "\n",
    "    Attributes:\n",
    "        file: Original PDF filename (for readtext compatibility)\n",
    "        text: Extracted text content\n",
    "        status: Processing status code\n",
    "        page_count: Number of pages processed\n",
    "        word_count: Approximate word count\n",
    "        text_length: Character count of extracted text\n",
    "        error_message: Error description if processing failed\n",
    "        metadata: Parsed document metadata\n",
    "        processing_time_seconds: Time taken to process\n",
    "    \"\"\"\n",
    "\n",
    "    file: str\n",
    "    text: str = \"\"\n",
    "    status: ProcessingStatus = ProcessingStatus.SKIPPED\n",
    "    page_count: int = 0\n",
    "    word_count: int = 0\n",
    "    text_length: int = 0\n",
    "    error_message: str | None = None\n",
    "    metadata: DocumentMetadata | None = None\n",
    "    processing_time_seconds: float = 0.0\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        \"\"\"Compute derived fields after initialization.\"\"\"\n",
    "        if self.text:\n",
    "            self.text_length = len(self.text)\n",
    "            self.word_count = len(self.text.split())\n",
    "\n",
    "    def to_dict(self) -> dict:\n",
    "        \"\"\"Convert to dictionary for DataFrame creation.\"\"\"\n",
    "        result = {\n",
    "            \"file\": self.file,\n",
    "            \"text\": self.text,\n",
    "            \"status\": self.status.name.lower(),\n",
    "            \"page_count\": self.page_count,\n",
    "            \"word_count\": self.word_count,\n",
    "            \"text_length\": self.text_length,\n",
    "            \"error\": self.error_message,\n",
    "            \"processing_time_seconds\": self.processing_time_seconds,\n",
    "        }\n",
    "\n",
    "        # Add metadata fields\n",
    "        if self.metadata:\n",
    "            result[\"municipality\"] = self.metadata.municipality\n",
    "            result[\"year\"] = self.metadata.year\n",
    "            result[\"maskad\"] = self.metadata.is_masked\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ProcessingConfig:\n",
    "    \"\"\"Configuration for OCR processing pipeline.\n",
    "\n",
    "    Attributes:\n",
    "        language: Tesseract language code(s)\n",
    "        dpi: Resolution for PDF to image conversion\n",
    "        min_text_length: Minimum characters for successful extraction\n",
    "        save_individual_txt: Whether to save per-document text files\n",
    "        workers: Number of parallel processing workers\n",
    "        tesseract_config: Additional Tesseract configuration\n",
    "    \"\"\"\n",
    "\n",
    "    language: str = DEFAULT_LANGUAGE\n",
    "    dpi: int = DEFAULT_DPI\n",
    "    min_text_length: int = DEFAULT_MIN_TEXT_LENGTH\n",
    "    save_individual_txt: bool = True\n",
    "    workers: int = DEFAULT_WORKERS\n",
    "    tesseract_config: str = TESSERACT_CONFIG\n",
    "\n",
    "    def validate(self) -> None:\n",
    "        \"\"\"Validate configuration parameters.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If any parameter is invalid\n",
    "        \"\"\"\n",
    "        if self.dpi < 72 or self.dpi > 600:\n",
    "            raise ValueError(f\"DPI must be between 72 and 600, got {self.dpi}\")\n",
    "        if self.min_text_length < 0:\n",
    "            raise ValueError(f\"min_text_length must be non-negative\")\n",
    "        if self.workers < 1:\n",
    "            raise ValueError(f\"workers must be at least 1\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Dependency Management\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "class DependencyError(Exception):\n",
    "    \"\"\"Raised when required dependencies are missing.\"\"\"\n",
    "\n",
    "    pass\n",
    "\n",
    "\n",
    "def check_python_dependencies() -> dict[str, bool]:\n",
    "    \"\"\"Check availability of required Python packages.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary mapping package names to availability status\n",
    "    \"\"\"\n",
    "    packages = {\n",
    "        \"pytesseract\": False,\n",
    "        \"pdf2image\": False,\n",
    "        \"PIL\": False,\n",
    "        \"pandas\": False,\n",
    "        \"tqdm\": False,\n",
    "        \"pyarrow\": False,\n",
    "    }\n",
    "\n",
    "    for package in packages:\n",
    "        try:\n",
    "            __import__(package)\n",
    "            packages[package] = True\n",
    "        except ImportError:\n",
    "            pass\n",
    "\n",
    "    return packages\n",
    "\n",
    "\n",
    "def verify_dependencies() -> None:\n",
    "    \"\"\"Verify all required dependencies are available.\n",
    "\n",
    "    Raises:\n",
    "        DependencyError: If any required package is missing\n",
    "    \"\"\"\n",
    "    status = check_python_dependencies()\n",
    "    missing = [pkg for pkg, available in status.items() if not available]\n",
    "\n",
    "    if missing:\n",
    "        install_cmd = f\"pip install {' '.join(missing)}\"\n",
    "        raise DependencyError(\n",
    "            f\"Missing required packages: {', '.join(missing)}\\n\"\n",
    "            f\"Install with: {install_cmd}\"\n",
    "        )\n",
    "\n",
    "\n",
    "def check_tesseract_installation() -> list[str]:\n",
    "    \"\"\"Check Tesseract OCR installation and verify Swedish language support.\n",
    "\n",
    "    Returns:\n",
    "        List of available languages\n",
    "\n",
    "    Raises:\n",
    "        DependencyError: If Tesseract is not installed or Swedish is unavailable\n",
    "    \"\"\"\n",
    "    import pytesseract\n",
    "\n",
    "    try:\n",
    "        version = pytesseract.get_tesseract_version()\n",
    "        languages = pytesseract.get_languages()\n",
    "\n",
    "        logging.info(f\"Tesseract version: {version}\")\n",
    "        logging.info(f\"Available languages: {', '.join(languages)}\")\n",
    "\n",
    "        if \"swe\" not in languages:\n",
    "            raise DependencyError(\n",
    "                \"Swedish language pack (swe) not found in Tesseract.\\n\"\n",
    "                \"All documents are in Swedish - cannot proceed without Swedish support.\\n\"\n",
    "                \"Install with:\\n\"\n",
    "                \"  Ubuntu/Debian: sudo apt-get install tesseract-ocr-swe\\n\"\n",
    "                \"  macOS: brew install tesseract-lang\"\n",
    "            )\n",
    "\n",
    "        logging.info(\"Swedish language support: OK\")\n",
    "        return languages\n",
    "\n",
    "    except pytesseract.TesseractNotFoundError as e:\n",
    "        raise DependencyError(\n",
    "            \"Tesseract OCR not found. Install with:\\n\"\n",
    "            \"  Ubuntu/Debian: sudo apt-get install tesseract-ocr tesseract-ocr-swe\\n\"\n",
    "            \"  macOS: brew install tesseract tesseract-lang\\n\"\n",
    "            \"  Windows: https://github.com/UB-Mannheim/tesseract/wiki\"\n",
    "        ) from e\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Image Preprocessing\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "class ImagePreprocessor(ABC):\n",
    "    \"\"\"Abstract base class for image preprocessing strategies.\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def process(self, image: \"Image.Image\") -> \"Image.Image\":\n",
    "        \"\"\"Apply preprocessing to an image.\n",
    "\n",
    "        Args:\n",
    "            image: PIL Image to process\n",
    "\n",
    "        Returns:\n",
    "            Processed PIL Image\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class GrayscalePreprocessor(ImagePreprocessor):\n",
    "    \"\"\"Convert image to grayscale.\"\"\"\n",
    "\n",
    "    def process(self, image: \"Image.Image\") -> \"Image.Image\":\n",
    "        \"\"\"Convert image to grayscale mode.\n",
    "\n",
    "        Args:\n",
    "            image: PIL Image (any mode)\n",
    "\n",
    "        Returns:\n",
    "            Grayscale PIL Image\n",
    "        \"\"\"\n",
    "        if image.mode != \"L\":\n",
    "            return image.convert(\"L\")\n",
    "        return image\n",
    "\n",
    "\n",
    "class ThresholdPreprocessor(ImagePreprocessor):\n",
    "    \"\"\"Apply binary threshold to image.\n",
    "\n",
    "    Useful for improving OCR on low-contrast scanned documents.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, threshold: int = 150):\n",
    "        \"\"\"Initialize with threshold value.\n",
    "\n",
    "        Args:\n",
    "            threshold: Pixel values above this become white (255),\n",
    "                      below become black (0)\n",
    "        \"\"\"\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def process(self, image: \"Image.Image\") -> \"Image.Image\":\n",
    "        \"\"\"Apply binary thresholding.\n",
    "\n",
    "        Args:\n",
    "            image: PIL Image (should be grayscale)\n",
    "\n",
    "        Returns:\n",
    "            Thresholded PIL Image\n",
    "        \"\"\"\n",
    "        return image.point(lambda x: 255 if x > self.threshold else 0)\n",
    "\n",
    "\n",
    "class CompositePreprocessor(ImagePreprocessor):\n",
    "    \"\"\"Apply multiple preprocessing steps in sequence.\"\"\"\n",
    "\n",
    "    def __init__(self, preprocessors: Sequence[ImagePreprocessor]):\n",
    "        \"\"\"Initialize with sequence of preprocessors.\n",
    "\n",
    "        Args:\n",
    "            preprocessors: Ordered sequence of preprocessors to apply\n",
    "        \"\"\"\n",
    "        self._preprocessors = list(preprocessors)\n",
    "\n",
    "    def process(self, image: \"Image.Image\") -> \"Image.Image\":\n",
    "        \"\"\"Apply all preprocessors in sequence.\n",
    "\n",
    "        Args:\n",
    "            image: PIL Image to process\n",
    "\n",
    "        Returns:\n",
    "            Processed PIL Image\n",
    "        \"\"\"\n",
    "        for preprocessor in self._preprocessors:\n",
    "            image = preprocessor.process(image)\n",
    "        return image\n",
    "\n",
    "\n",
    "def get_default_preprocessor() -> ImagePreprocessor:\n",
    "    \"\"\"Get the default image preprocessing pipeline.\n",
    "\n",
    "    Returns:\n",
    "        Configured ImagePreprocessor instance\n",
    "    \"\"\"\n",
    "    return GrayscalePreprocessor()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# OCR Engine\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "class OCREngine:\n",
    "    \"\"\"Wrapper for Tesseract OCR operations.\n",
    "\n",
    "    Handles PDF to image conversion and text extraction with\n",
    "    configurable preprocessing and error handling.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: ProcessingConfig,\n",
    "        preprocessor: ImagePreprocessor | None = None,\n",
    "    ):\n",
    "        \"\"\"Initialize OCR engine.\n",
    "\n",
    "        Args:\n",
    "            config: Processing configuration\n",
    "            preprocessor: Image preprocessor (uses default if None)\n",
    "        \"\"\"\n",
    "        self._config = config\n",
    "        self._preprocessor = preprocessor or get_default_preprocessor()\n",
    "\n",
    "        # Lazy imports to allow dependency checking first\n",
    "        import pytesseract\n",
    "        from pdf2image import convert_from_path\n",
    "        from PIL import Image\n",
    "\n",
    "        self._pytesseract = pytesseract\n",
    "        self._convert_from_path = convert_from_path\n",
    "        self._Image = Image\n",
    "\n",
    "    def extract_text(self, pdf_path: Path) -> tuple[str, int]:\n",
    "        \"\"\"Extract text from a PDF using OCR.\n",
    "\n",
    "        Args:\n",
    "            pdf_path: Path to the PDF file\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (extracted_text, page_count)\n",
    "\n",
    "        Raises:\n",
    "            RuntimeError: If PDF conversion fails\n",
    "            IOError: If file cannot be read\n",
    "        \"\"\"\n",
    "        if not pdf_path.exists():\n",
    "            raise IOError(f\"PDF file not found: {pdf_path}\")\n",
    "\n",
    "        # Convert PDF pages to images\n",
    "        try:\n",
    "            images = self._convert_from_path(\n",
    "                str(pdf_path),\n",
    "                dpi=self._config.dpi,\n",
    "                thread_count=1,  # Avoid nested parallelism\n",
    "            )\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to convert PDF to images: {e}\") from e\n",
    "\n",
    "        # Process each page\n",
    "        page_texts = []\n",
    "        for page_num, image in enumerate(images, start=1):\n",
    "            page_text = self._ocr_single_page(image, page_num)\n",
    "            page_texts.append(page_text)\n",
    "\n",
    "        full_text = \"\\n\\n\".join(page_texts)\n",
    "        return full_text, len(images)\n",
    "\n",
    "    def _ocr_single_page(\n",
    "        self,\n",
    "        image: \"Image.Image\",\n",
    "        page_num: int,\n",
    "    ) -> str:\n",
    "        \"\"\"Perform OCR on a single page image.\n",
    "\n",
    "        Args:\n",
    "            image: PIL Image of the page\n",
    "            page_num: Page number for logging\n",
    "\n",
    "        Returns:\n",
    "            Extracted text with page header\n",
    "        \"\"\"\n",
    "        # Apply preprocessing\n",
    "        processed_image = self._preprocessor.process(image)\n",
    "\n",
    "        try:\n",
    "            text = self._pytesseract.image_to_string(\n",
    "                processed_image,\n",
    "                lang=self._config.language,\n",
    "                config=self._config.tesseract_config,\n",
    "            )\n",
    "            return f\"--- Page {page_num} ---\\n{text}\"\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"OCR failed on page {page_num}: {e}\")\n",
    "            return f\"--- Page {page_num} ---\\n[OCR ERROR: {e}]\"\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# File Discovery\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def discover_pdf_files(\n",
    "    input_dir: Path,\n",
    "    file_list: Path | None = None,\n",
    ") -> tuple[list[Path], list[str]]:\n",
    "    \"\"\"Discover PDF files to process.\n",
    "\n",
    "    Args:\n",
    "        input_dir: Directory containing PDF files\n",
    "        file_list: Optional text file with specific filenames\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (found_files, missing_filenames)\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If input_dir doesn't exist\n",
    "    \"\"\"\n",
    "    if not input_dir.exists():\n",
    "        raise ValueError(f\"Input directory does not exist: {input_dir}\")\n",
    "\n",
    "    if not input_dir.is_dir():\n",
    "        raise ValueError(f\"Input path is not a directory: {input_dir}\")\n",
    "\n",
    "    if file_list and file_list.exists():\n",
    "        return _discover_from_list(input_dir, file_list)\n",
    "    else:\n",
    "        return _discover_from_directory(input_dir)\n",
    "\n",
    "\n",
    "def _discover_from_list(\n",
    "    input_dir: Path,\n",
    "    file_list: Path,\n",
    ") -> tuple[list[Path], list[str]]:\n",
    "    \"\"\"Discover PDFs from a file list.\n",
    "\n",
    "    Args:\n",
    "        input_dir: Base directory for PDF files\n",
    "        file_list: Text file with filenames (one per line)\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (found_files, missing_filenames)\n",
    "    \"\"\"\n",
    "    with open(file_list, \"r\", encoding=\"utf-8\") as f:\n",
    "        filenames = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "    found = []\n",
    "    missing = []\n",
    "\n",
    "    for filename in filenames:\n",
    "        pdf_path = input_dir / filename\n",
    "        if pdf_path.exists():\n",
    "            found.append(pdf_path)\n",
    "        else:\n",
    "            missing.append(filename)\n",
    "\n",
    "    return found, missing\n",
    "\n",
    "\n",
    "def _discover_from_directory(input_dir: Path) -> tuple[list[Path], list[str]]:\n",
    "    \"\"\"Discover all PDFs in a directory.\n",
    "\n",
    "    Args:\n",
    "        input_dir: Directory to scan\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (found_files, empty_list)\n",
    "    \"\"\"\n",
    "    pdf_files = [\n",
    "        f for f in input_dir.iterdir() if f.suffix in PDF_EXTENSIONS and f.is_file()\n",
    "    ]\n",
    "    return sorted(pdf_files), []\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Document Processor\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def process_single_document(\n",
    "    pdf_path: Path,\n",
    "    config: ProcessingConfig,\n",
    "    output_dir: Path | None = None,\n",
    ") -> OCRResult:\n",
    "    \"\"\"Process a single PDF document.\n",
    "\n",
    "    This function is designed to be called in parallel workers.\n",
    "\n",
    "    Args:\n",
    "        pdf_path: Path to the PDF file\n",
    "        config: Processing configuration\n",
    "        output_dir: Optional directory for individual text files\n",
    "\n",
    "    Returns:\n",
    "        OCRResult with processing outcome\n",
    "    \"\"\"\n",
    "    import time\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    # Parse metadata from filename\n",
    "    metadata = DocumentMetadata.from_filename(pdf_path.name)\n",
    "\n",
    "    result = OCRResult(\n",
    "        file=pdf_path.name,\n",
    "        metadata=metadata,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Initialize OCR engine\n",
    "        engine = OCREngine(config)\n",
    "\n",
    "        # Extract text\n",
    "        text, page_count = engine.extract_text(pdf_path)\n",
    "\n",
    "        result.text = text\n",
    "        result.page_count = page_count\n",
    "        result.status = ProcessingStatus.SUCCESS\n",
    "\n",
    "        # Check minimum length threshold\n",
    "        if result.text_length < config.min_text_length:\n",
    "            result.status = ProcessingStatus.FAILED_SHORT\n",
    "\n",
    "        # Save individual text file if requested\n",
    "        if (\n",
    "            output_dir\n",
    "            and config.save_individual_txt\n",
    "            and result.status == ProcessingStatus.SUCCESS\n",
    "        ):\n",
    "            _save_text_file(output_dir, pdf_path.stem, text)\n",
    "\n",
    "    except IOError as e:\n",
    "        result.status = ProcessingStatus.FAILED_IO\n",
    "        result.error_message = str(e)\n",
    "        logging.error(f\"IO error processing {pdf_path.name}: {e}\")\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        result.status = ProcessingStatus.FAILED_CONVERSION\n",
    "        result.error_message = str(e)\n",
    "        logging.error(f\"Conversion error processing {pdf_path.name}: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        result.status = ProcessingStatus.FAILED_OCR\n",
    "        result.error_message = str(e)\n",
    "        logging.exception(f\"Unexpected error processing {pdf_path.name}\")\n",
    "\n",
    "    finally:\n",
    "        result.processing_time_seconds = time.perf_counter() - start_time\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def _save_text_file(output_dir: Path, stem: str, text: str) -> None:\n",
    "    \"\"\"Save extracted text to individual file.\n",
    "\n",
    "    Args:\n",
    "        output_dir: Output directory\n",
    "        stem: Filename without extension\n",
    "        text: Text content to save\n",
    "    \"\"\"\n",
    "    txt_path = output_dir / f\"{stem}.txt\"\n",
    "    try:\n",
    "        txt_path.write_text(text, encoding=\"utf-8\")\n",
    "    except IOError as e:\n",
    "        logging.warning(f\"Failed to save text file {txt_path}: {e}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Batch Processor\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "class BatchProcessor:\n",
    "    \"\"\"Orchestrates batch processing of multiple PDF documents.\n",
    "\n",
    "    Supports both sequential and parallel processing modes.\n",
    "    \n",
    "    Note:\n",
    "        Parallel processing uses multiprocessing which may have limitations\n",
    "        in Jupyter notebooks. Sequential processing is recommended for Jupyter.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: ProcessingConfig,\n",
    "        output_dir: Path,\n",
    "    ):\n",
    "        \"\"\"Initialize batch processor.\n",
    "\n",
    "        Args:\n",
    "            config: Processing configuration\n",
    "            output_dir: Directory for output files\n",
    "        \"\"\"\n",
    "        self._config = config\n",
    "        self._output_dir = output_dir\n",
    "        self._results: list[OCRResult] = []\n",
    "\n",
    "    def process_files(self, pdf_files: Sequence[Path]) -> list[OCRResult]:\n",
    "        \"\"\"Process multiple PDF files.\n",
    "\n",
    "        Args:\n",
    "            pdf_files: Sequence of PDF paths to process\n",
    "\n",
    "        Returns:\n",
    "            List of OCRResult objects\n",
    "        \"\"\"\n",
    "        # Use tqdm.auto for Jupyter compatibility\n",
    "        from tqdm.auto import tqdm\n",
    "\n",
    "        self._results = []\n",
    "\n",
    "        # Warn about parallel processing in Jupyter\n",
    "        if self._config.workers > 1 and IS_JUPYTER:\n",
    "            logging.warning(\n",
    "                \"Parallel processing in Jupyter may cause issues. \"\n",
    "                \"Consider using workers=1 for reliability.\"\n",
    "            )\n",
    "\n",
    "        if self._config.workers == 1:\n",
    "            # Sequential processing (recommended for Jupyter)\n",
    "            for pdf_path in tqdm(pdf_files, desc=\"Processing PDFs\"):\n",
    "                result = process_single_document(\n",
    "                    pdf_path,\n",
    "                    self._config,\n",
    "                    self._output_dir,\n",
    "                )\n",
    "                self._results.append(result)\n",
    "                self._log_result(result)\n",
    "        else:\n",
    "            # Parallel processing\n",
    "            self._results = self._process_parallel(pdf_files)\n",
    "\n",
    "        return self._results\n",
    "\n",
    "    def _process_parallel(self, pdf_files: Sequence[Path]) -> list[OCRResult]:\n",
    "        \"\"\"Process files in parallel using ProcessPoolExecutor.\n",
    "\n",
    "        Args:\n",
    "            pdf_files: Sequence of PDF paths\n",
    "\n",
    "        Returns:\n",
    "            List of OCRResult objects\n",
    "            \n",
    "        Note:\n",
    "            May not work correctly in Jupyter notebooks due to \n",
    "            multiprocessing limitations. Use sequential processing instead.\n",
    "        \"\"\"\n",
    "        from tqdm.auto import tqdm\n",
    "\n",
    "        results = []\n",
    "\n",
    "        with ProcessPoolExecutor(max_workers=self._config.workers) as executor:\n",
    "            # Submit all jobs\n",
    "            future_to_path = {\n",
    "                executor.submit(\n",
    "                    process_single_document,\n",
    "                    pdf_path,\n",
    "                    self._config,\n",
    "                    self._output_dir,\n",
    "                ): pdf_path\n",
    "                for pdf_path in pdf_files\n",
    "            }\n",
    "\n",
    "            # Collect results with progress bar\n",
    "            for future in tqdm(\n",
    "                as_completed(future_to_path),\n",
    "                total=len(pdf_files),\n",
    "                desc=\"Processing PDFs\",\n",
    "            ):\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    results.append(result)\n",
    "                    self._log_result(result)\n",
    "                except Exception as e:\n",
    "                    pdf_path = future_to_path[future]\n",
    "                    logging.error(f\"Worker failed for {pdf_path.name}: {e}\")\n",
    "                    results.append(\n",
    "                        OCRResult(\n",
    "                            file=pdf_path.name,\n",
    "                            status=ProcessingStatus.FAILED_OCR,\n",
    "                            error_message=str(e),\n",
    "                            metadata=DocumentMetadata.from_filename(pdf_path.name),\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "        return results\n",
    "\n",
    "    def _log_result(self, result: OCRResult) -> None:\n",
    "        \"\"\"Log processing result.\n",
    "\n",
    "        Args:\n",
    "            result: OCRResult to log\n",
    "        \"\"\"\n",
    "        if result.status.is_success():\n",
    "            logging.info(\n",
    "                f\"Success: {result.file} \"\n",
    "                f\"({result.page_count} pages, {result.word_count} words)\"\n",
    "            )\n",
    "        else:\n",
    "            logging.warning(\n",
    "                f\"Failed: {result.file} \"\n",
    "                f\"(status={result.status.name}, error={result.error_message})\"\n",
    "            )\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Output Generation\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ProcessingSummary:\n",
    "    \"\"\"Summary statistics for batch processing.\"\"\"\n",
    "\n",
    "    total_files: int = 0\n",
    "    successful: int = 0\n",
    "    failed_short: int = 0\n",
    "    failed_other: int = 0\n",
    "    total_pages: int = 0\n",
    "    total_words: int = 0\n",
    "    total_time_seconds: float = 0.0\n",
    "\n",
    "    @classmethod\n",
    "    def from_results(cls, results: Sequence[OCRResult]) -> ProcessingSummary:\n",
    "        \"\"\"Compute summary from processing results.\n",
    "\n",
    "        Args:\n",
    "            results: Sequence of OCRResult objects\n",
    "\n",
    "        Returns:\n",
    "            ProcessingSummary instance\n",
    "        \"\"\"\n",
    "        summary = cls(total_files=len(results))\n",
    "\n",
    "        for result in results:\n",
    "            if result.status == ProcessingStatus.SUCCESS:\n",
    "                summary.successful += 1\n",
    "                summary.total_pages += result.page_count\n",
    "                summary.total_words += result.word_count\n",
    "            elif result.status == ProcessingStatus.FAILED_SHORT:\n",
    "                summary.failed_short += 1\n",
    "            else:\n",
    "                summary.failed_other += 1\n",
    "\n",
    "            summary.total_time_seconds += result.processing_time_seconds\n",
    "\n",
    "        return summary\n",
    "\n",
    "    def log(self) -> None:\n",
    "        \"\"\"Log summary statistics.\"\"\"\n",
    "        logging.info(\"=\" * 60)\n",
    "        logging.info(\"Processing Summary\")\n",
    "        logging.info(\"=\" * 60)\n",
    "        logging.info(f\"Total files processed: {self.total_files}\")\n",
    "        logging.info(f\"Successful extractions: {self.successful}\")\n",
    "        logging.info(f\"Failed (too short): {self.failed_short}\")\n",
    "        logging.info(f\"Failed (other errors): {self.failed_other}\")\n",
    "        logging.info(f\"Total pages processed: {self.total_pages}\")\n",
    "        logging.info(f\"Total words extracted: {self.total_words:,}\")\n",
    "        logging.info(f\"Total processing time: {self.total_time_seconds:.1f}s\")\n",
    "\n",
    "\n",
    "class OutputWriter:\n",
    "    \"\"\"Handles writing processing results to various formats.\n",
    "    \n",
    "    All outputs are in Parquet format for efficient Python workflows.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_dir: Path, min_text_length: int):\n",
    "        \"\"\"Initialize output writer.\n",
    "\n",
    "        Args:\n",
    "            output_dir: Directory for output files\n",
    "            min_text_length: Threshold for success classification\n",
    "        \"\"\"\n",
    "        self._output_dir = output_dir\n",
    "        self._min_text_length = min_text_length\n",
    "\n",
    "    def write_all(self, results: Sequence[OCRResult]) -> dict[str, Path]:\n",
    "        \"\"\"Write all output files.\n",
    "\n",
    "        Args:\n",
    "            results: Processing results to write\n",
    "\n",
    "        Returns:\n",
    "            Dictionary mapping output type to file path\n",
    "        \"\"\"\n",
    "        import pandas as pd\n",
    "\n",
    "        # Convert results to DataFrame\n",
    "        df = pd.DataFrame([r.to_dict() for r in results])\n",
    "\n",
    "        output_paths = {}\n",
    "\n",
    "        # 1. readtext-compatible format (primary output) - Parquet only\n",
    "        output_paths.update(self._write_readtext_format(df))\n",
    "\n",
    "        # 2. Full results with metadata\n",
    "        output_paths.update(self._write_full_results(df))\n",
    "\n",
    "        # 3. Summary CSV (without text)\n",
    "        output_paths.update(self._write_summary(df))\n",
    "\n",
    "        # 4. Failed files list\n",
    "        output_paths.update(self._write_failed_list(df))\n",
    "\n",
    "        return output_paths\n",
    "\n",
    "    def _write_readtext_format(self, df: \"pd.DataFrame\") -> dict[str, Path]:\n",
    "        \"\"\"Write readtext-compatible output file (Parquet format).\n",
    "\n",
    "        Args:\n",
    "            df: Full results DataFrame\n",
    "\n",
    "        Returns:\n",
    "            Dictionary of output paths\n",
    "        \"\"\"\n",
    "        # Filter to successful only, keep only file and text columns\n",
    "        df_readtext = df[df[\"status\"] == \"success\"][[\"file\", \"text\"]].copy()\n",
    "        df_readtext = df_readtext.reset_index(drop=True)\n",
    "\n",
    "        paths = {}\n",
    "\n",
    "        # Parquet format (primary output for Python workflows)\n",
    "        parquet_path = self._output_dir / \"ocr_readtext_format.parquet\"\n",
    "        df_readtext.to_parquet(parquet_path, index=False)\n",
    "        paths[\"readtext_parquet\"] = parquet_path\n",
    "        logging.info(f\"Saved Parquet (readtext format): {parquet_path}\")\n",
    "        logging.info(f\"  → {len(df_readtext)} documents ready for Stanza pipeline\")\n",
    "\n",
    "        return paths\n",
    "\n",
    "    def _write_full_results(self, df: \"pd.DataFrame\") -> dict[str, Path]:\n",
    "        \"\"\"Write full results with all metadata.\n",
    "\n",
    "        Args:\n",
    "            df: Full results DataFrame\n",
    "\n",
    "        Returns:\n",
    "            Dictionary of output paths\n",
    "        \"\"\"\n",
    "        parquet_path = self._output_dir / \"ocr_full_results.parquet\"\n",
    "        df.to_parquet(parquet_path, index=False)\n",
    "        logging.info(f\"Saved full results: {parquet_path}\")\n",
    "        return {\"full_results\": parquet_path}\n",
    "\n",
    "    def _write_summary(self, df: \"pd.DataFrame\") -> dict[str, Path]:\n",
    "        \"\"\"Write summary CSV without text content.\n",
    "\n",
    "        Args:\n",
    "            df: Full results DataFrame\n",
    "\n",
    "        Returns:\n",
    "            Dictionary of output paths\n",
    "        \"\"\"\n",
    "        csv_path = self._output_dir / \"ocr_results_summary.csv\"\n",
    "        df_summary = df.drop(columns=[\"text\"], errors=\"ignore\")\n",
    "        df_summary.to_csv(csv_path, index=False, encoding=\"utf-8\")\n",
    "        logging.info(f\"Saved summary CSV: {csv_path}\")\n",
    "        return {\"summary_csv\": csv_path}\n",
    "\n",
    "    def _write_failed_list(self, df: \"pd.DataFrame\") -> dict[str, Path]:\n",
    "        \"\"\"Write list of files that still failed.\n",
    "\n",
    "        Args:\n",
    "            df: Full results DataFrame\n",
    "\n",
    "        Returns:\n",
    "            Dictionary of output paths\n",
    "        \"\"\"\n",
    "        failed_files = df[df[\"status\"] != \"success\"][\"file\"].tolist()\n",
    "\n",
    "        if not failed_files:\n",
    "            return {}\n",
    "\n",
    "        failed_path = self._output_dir / \"still_failed_files.txt\"\n",
    "        failed_path.write_text(\"\\n\".join(failed_files) + \"\\n\", encoding=\"utf-8\")\n",
    "        logging.info(f\"Saved failed files list: {failed_path}\")\n",
    "        return {\"failed_list\": failed_path}\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Logging Setup\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def setup_logging(\n",
    "    output_dir: Path | None = None,\n",
    "    level: int = logging.INFO,\n",
    "    jupyter_mode: bool = False,\n",
    ") -> logging.Logger:\n",
    "    \"\"\"Configure logging to file and console.\n",
    "\n",
    "    Args:\n",
    "        output_dir: Directory for log file (None for no file logging)\n",
    "        level: Logging level\n",
    "        jupyter_mode: If True, configure for Jupyter notebook display\n",
    "\n",
    "    Returns:\n",
    "        Configured logger instance\n",
    "    \"\"\"\n",
    "    # Clear any existing handlers\n",
    "    root_logger = logging.getLogger()\n",
    "    root_logger.handlers.clear()\n",
    "    \n",
    "    handlers = []\n",
    "    \n",
    "    # Console/Jupyter handler\n",
    "    console_handler = logging.StreamHandler(sys.stdout)\n",
    "    console_handler.setLevel(level)\n",
    "    handlers.append(console_handler)\n",
    "    \n",
    "    # File handler (if output_dir provided)\n",
    "    if output_dir is not None:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        log_file = output_dir / f\"ocr_processing_{timestamp}.log\"\n",
    "        file_handler = logging.FileHandler(log_file, encoding=\"utf-8\")\n",
    "        file_handler.setLevel(level)\n",
    "        handlers.append(file_handler)\n",
    "\n",
    "    # Configure logging\n",
    "    logging.basicConfig(\n",
    "        level=level,\n",
    "        format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "        handlers=handlers,\n",
    "        force=True,  # Override any existing configuration\n",
    "    )\n",
    "\n",
    "    return logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Main Pipeline\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def run_pipeline(\n",
    "    input_dir: Path,\n",
    "    output_dir: Path,\n",
    "    file_list: Path | None = None,\n",
    "    config: ProcessingConfig | None = None,\n",
    ") -> tuple[list[OCRResult], ProcessingSummary]:\n",
    "    \"\"\"Run the complete OCR processing pipeline.\n",
    "\n",
    "    Args:\n",
    "        input_dir: Directory containing PDF files\n",
    "        output_dir: Directory for output files\n",
    "        file_list: Optional text file with specific filenames\n",
    "        config: Processing configuration (uses defaults if None)\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (results_list, processing_summary)\n",
    "\n",
    "    Raises:\n",
    "        DependencyError: If required dependencies are missing (including Swedish)\n",
    "        ValueError: If input validation fails\n",
    "    \"\"\"\n",
    "    # Use default config if not provided\n",
    "    if config is None:\n",
    "        config = ProcessingConfig()\n",
    "\n",
    "    # Validate configuration\n",
    "    config.validate()\n",
    "\n",
    "    # Create output directory\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Set up logging (with Jupyter detection)\n",
    "    logger = setup_logging(output_dir, jupyter_mode=IS_JUPYTER)\n",
    "    logger.info(\"Starting OCR processing pipeline\")\n",
    "    logger.info(f\"Input directory: {input_dir}\")\n",
    "    logger.info(f\"Output directory: {output_dir}\")\n",
    "    logger.info(f\"Configuration: {config}\")\n",
    "    \n",
    "    if IS_JUPYTER:\n",
    "        logger.info(\"Jupyter environment detected\")\n",
    "\n",
    "    # Verify dependencies (will raise if Swedish not available)\n",
    "    verify_dependencies()\n",
    "    languages = check_tesseract_installation()  # Raises if Swedish missing\n",
    "\n",
    "    # Discover PDF files\n",
    "    pdf_files, missing_files = discover_pdf_files(input_dir, file_list)\n",
    "\n",
    "    if missing_files:\n",
    "        logger.warning(f\"Missing {len(missing_files)} files from list:\")\n",
    "        for filename in missing_files[:10]:\n",
    "            logger.warning(f\"  - {filename}\")\n",
    "        if len(missing_files) > 10:\n",
    "            logger.warning(f\"  ... and {len(missing_files) - 10} more\")\n",
    "\n",
    "    if not pdf_files:\n",
    "        logger.error(\"No PDF files found to process\")\n",
    "        return [], ProcessingSummary()\n",
    "\n",
    "    logger.info(f\"Found {len(pdf_files)} PDF files to process\")\n",
    "\n",
    "    # Process files\n",
    "    processor = BatchProcessor(config, output_dir)\n",
    "    results = processor.process_files(pdf_files)\n",
    "\n",
    "    # Generate summary\n",
    "    summary = ProcessingSummary.from_results(results)\n",
    "    summary.log()\n",
    "\n",
    "    # Write output files\n",
    "    writer = OutputWriter(output_dir, config.min_text_length)\n",
    "    output_paths = writer.write_all(results)\n",
    "\n",
    "    logger.info(\"Pipeline complete\")\n",
    "\n",
    "    return results, summary\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CLI Interface\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "def create_argument_parser() -> argparse.ArgumentParser:\n",
    "    \"\"\"Create and configure argument parser.\n",
    "\n",
    "    Returns:\n",
    "        Configured ArgumentParser instance\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser(\n",
    "        prog=\"ocr_swedish_pdfs\",\n",
    "        description=\"OCR processing for Swedish RSA PDF documents\",\n",
    "        formatter_class=argparse.RawDescriptionHelpFormatter,\n",
    "        epilog=\"\"\"\n",
    "Examples:\n",
    "    # Process all PDFs in a directory\n",
    "    python ocr_swedish_pdfs.py -i ./pdfs -o ./output\n",
    "\n",
    "    # Process specific files from a list\n",
    "    python ocr_swedish_pdfs.py -i ./pdfs -o ./output -f failed_files.txt\n",
    "\n",
    "    # Use only Swedish language (faster, recommended)\n",
    "    python ocr_swedish_pdfs.py -i ./pdfs -o ./output --lang swe\n",
    "\n",
    "    # Check Tesseract installation only\n",
    "    python ocr_swedish_pdfs.py --check-only\n",
    "\n",
    "Output Files:\n",
    "    ocr_readtext_format.parquet - Primary output matching readtext format (file, text)\n",
    "    ocr_full_results.parquet    - Full results with metadata\n",
    "    ocr_results_summary.csv     - Summary without text content\n",
    "    still_failed_files.txt      - Files that still failed OCR\n",
    "\n",
    "Note:\n",
    "    Swedish language support in Tesseract is REQUIRED. The script will fail\n",
    "    if tesseract-ocr-swe is not installed.\n",
    "    \n",
    "    For Jupyter Lab usage, import and use run_pipeline() directly instead.\n",
    "        \"\"\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--input_dir\",\n",
    "        \"-i\",\n",
    "        type=Path,\n",
    "        help=\"Directory containing PDF files\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--output_dir\",\n",
    "        \"-o\",\n",
    "        type=Path,\n",
    "        help=\"Directory to save output files\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--file_list\",\n",
    "        \"-f\",\n",
    "        type=Path,\n",
    "        default=None,\n",
    "        help=\"Text file with specific PDF filenames to process (one per line)\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--lang\",\n",
    "        \"-l\",\n",
    "        type=str,\n",
    "        default=DEFAULT_LANGUAGE,\n",
    "        help=f\"Tesseract language code(s) (default: {DEFAULT_LANGUAGE})\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--dpi\",\n",
    "        type=int,\n",
    "        default=DEFAULT_DPI,\n",
    "        help=f\"DPI for PDF to image conversion (default: {DEFAULT_DPI})\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--min-length\",\n",
    "        type=int,\n",
    "        default=DEFAULT_MIN_TEXT_LENGTH,\n",
    "        help=f\"Minimum text length for success (default: {DEFAULT_MIN_TEXT_LENGTH})\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--workers\",\n",
    "        \"-w\",\n",
    "        type=int,\n",
    "        default=DEFAULT_WORKERS,\n",
    "        help=f\"Number of parallel workers (default: {DEFAULT_WORKERS})\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--no-individual\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Do not save individual text files for each PDF\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--check-only\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Only check Tesseract installation, do not process files\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--verbose\",\n",
    "        \"-v\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Enable verbose (debug) logging\",\n",
    "    )\n",
    "\n",
    "    return parser\n",
    "\n",
    "\n",
    "def main() -> int:\n",
    "    \"\"\"Main entry point for CLI.\n",
    "\n",
    "    Returns:\n",
    "        Exit code (0 for success, non-zero for errors)\n",
    "    \"\"\"\n",
    "    parser = create_argument_parser()\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Handle check-only mode\n",
    "    if args.check_only:\n",
    "        try:\n",
    "            verify_dependencies()\n",
    "            languages = check_tesseract_installation()\n",
    "            print(\"\\nDependency check passed!\")\n",
    "            print(f\"Swedish language support: Yes\")\n",
    "            print(f\"Available languages: {', '.join(languages)}\")\n",
    "            return 0\n",
    "        except DependencyError as e:\n",
    "            print(f\"\\nDependency check failed:\\n{e}\")\n",
    "            return 1\n",
    "\n",
    "    # Validate required arguments\n",
    "    if not args.input_dir:\n",
    "        parser.error(\"--input_dir is required unless using --check-only\")\n",
    "    if not args.output_dir:\n",
    "        parser.error(\"--output_dir is required unless using --check-only\")\n",
    "\n",
    "    # Validate input directory\n",
    "    if not args.input_dir.exists():\n",
    "        print(f\"Error: Input directory does not exist: {args.input_dir}\")\n",
    "        return 1\n",
    "\n",
    "    # Build configuration\n",
    "    config = ProcessingConfig(\n",
    "        language=args.lang,\n",
    "        dpi=args.dpi,\n",
    "        min_text_length=args.min_length,\n",
    "        save_individual_txt=not args.no_individual,\n",
    "        workers=args.workers,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        # Run pipeline\n",
    "        results, summary = run_pipeline(\n",
    "            input_dir=args.input_dir,\n",
    "            output_dir=args.output_dir,\n",
    "            file_list=args.file_list,\n",
    "            config=config,\n",
    "        )\n",
    "\n",
    "        # Print final summary\n",
    "        print(f\"\\n{'=' * 60}\")\n",
    "        print(\"OCR Processing Complete!\")\n",
    "        print(f\"{'=' * 60}\")\n",
    "        print(f\"Output directory: {args.output_dir}\")\n",
    "        print(f\"Files processed: {summary.total_files}\")\n",
    "        print(f\"Successful: {summary.successful}\")\n",
    "        print(f\"Failed (short): {summary.failed_short}\")\n",
    "        print(f\"Failed (other): {summary.failed_other}\")\n",
    "\n",
    "        return 0 if summary.successful > 0 else 1\n",
    "\n",
    "    except DependencyError as e:\n",
    "        print(f\"\\nDependency error:\\n{e}\")\n",
    "        return 1\n",
    "\n",
    "    except ValueError as e:\n",
    "        print(f\"\\nConfiguration error: {e}\")\n",
    "        return 1\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nProcessing interrupted by user\")\n",
    "        return 130\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.exception(\"Unexpected error during processing\")\n",
    "        print(f\"\\nUnexpected error: {e}\")\n",
    "        return 1\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sys.exit(main())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
