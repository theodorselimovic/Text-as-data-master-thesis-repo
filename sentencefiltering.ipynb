{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9b0c30e-2001-453a-bb85-fb2a2ad4f253",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "from typing import List, Dict, Tuple, Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b31e69e-8db9-49e2-b053-f1336a9aa07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Method for combining word vectors into sentence vector\n",
    "# Options: 'mean' (average) or 'sum' (addition)\n",
    "AGGREGATION_METHOD = 'mean'  # Mean is standard for sentence embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "720a5c9a-c104-4b24-9e8f-d2d1be3a374b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LOADING DATA\n",
      "================================================================================\n",
      "resilience: 109 terms\n",
      "risk: 114 terms\n",
      "\n",
      "Total unique target terms across all categories: 219\n",
      "\n",
      "Loading sentence data...\n",
      "Loaded 260393 sentences\n",
      "Columns: ['doc_id', 'municipality', 'year', 'maskad', 'sentence_id', 'sentence_text', 'word_count']\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PART 1: LOAD DATA\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"LOADING DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load your expanded terms\n",
    "df_expanded = pd.read_csv('expanded_terms_lemmatized.csv')\n",
    "\n",
    "# Create lookup dictionary: category -> set of lemmas\n",
    "category_lemmas = {}\n",
    "all_target_lemmas = set()\n",
    "\n",
    "for category in df_expanded['category'].unique():\n",
    "    lemmas = set(df_expanded[df_expanded['category'] == category]['lemma'].tolist())\n",
    "    category_lemmas[category] = lemmas\n",
    "    all_target_lemmas.update(lemmas)\n",
    "    print(f\"{category}: {len(lemmas)} terms\")\n",
    "\n",
    "print(f\"\\nTotal unique target terms across all categories: {len(all_target_lemmas)}\")\n",
    "\n",
    "# Load your sentence-level data\n",
    "print(\"\\nLoading sentence data...\")\n",
    "df_sentences = pd.read_parquet('sentences_lemmatized.parquet')\n",
    "print(f\"Loaded {len(df_sentences)} sentences\")\n",
    "print(f\"Columns: {df_sentences.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8308660-a9ca-4514-8f85-3d60c41bae10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FILTERING SENTENCES WITH TARGET TERMS\n",
      "================================================================================\n",
      "Identifying sentences with target terms...\n",
      "Sentences containing target terms: 41311 (15.86%)\n",
      "\n",
      "Sentences by number of categories:\n",
      "categories\n",
      "1    35052\n",
      "2     6259\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PART 2: FILTER SENTENCES CONTAINING EXPANDED TERMS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FILTERING SENTENCES WITH TARGET TERMS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def find_target_terms_in_sentence(sentence_text: str, all_target_lemmas: Set[str]) -> List[str]:\n",
    "    \"\"\"Find which target terms appear in a sentence.\"\"\"\n",
    "    words = sentence_text.split()\n",
    "    found_terms = [word for word in words if word in all_target_lemmas]\n",
    "    return found_terms\n",
    "\n",
    "def get_categories_for_terms(terms: List[str], category_lemmas: Dict) -> List[str]:\n",
    "    \"\"\"Get all categories that these terms belong to.\"\"\"\n",
    "    categories = set()\n",
    "    for term in terms:\n",
    "        for category, lemmas in category_lemmas.items():\n",
    "            if term in lemmas:\n",
    "                categories.add(category)\n",
    "    return list(categories)\n",
    "\n",
    "# Find sentences containing target terms\n",
    "print(\"Identifying sentences with target terms...\")\n",
    "df_sentences['target_terms'] = df_sentences['sentence_text'].apply(\n",
    "    lambda x: find_target_terms_in_sentence(x, all_target_lemmas)\n",
    ")\n",
    "\n",
    "# Filter to only sentences with at least one target term\n",
    "df_with_targets = df_sentences[df_sentences['target_terms'].apply(len) > 0].copy()\n",
    "print(f\"Sentences containing target terms: {len(df_with_targets)} ({len(df_with_targets)/len(df_sentences)*100:.2f}%)\")\n",
    "\n",
    "# Assign categories to each sentence\n",
    "df_with_targets['categories'] = df_with_targets['target_terms'].apply(\n",
    "    lambda terms: get_categories_for_terms(terms, category_lemmas)\n",
    ")\n",
    "\n",
    "print(f\"\\nSentences by number of categories:\")\n",
    "print(df_with_targets['categories'].apply(len).value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67b7401e-bdf4-46ec-8fd8-c93829f4afbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "LOADING FASTTEXT MODEL\n",
      "================================================================================\n",
      "Loading FastText model (this may take a few minutes)...\n",
      "Model loaded with 300 dimensions\n",
      "Model has subword information - can handle OOV words!\n",
      "\n",
      "Vectorizing sentences using 'mean' aggregation...\n",
      "This may take a few minutes...\n",
      "\n",
      "Successfully vectorized 41311 sentences\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PART 3: LOAD FASTTEXT MODEL AND VECTORIZE SENTENCES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LOADING FASTTEXT MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"Loading FastText model (this may take a few minutes)...\")\n",
    "model = fasttext.load_model('/Users/theodorselimovic/Sciences Po/Material/word vectors/cc.sv.300.bin')\n",
    "print(f\"Model loaded with {model.get_dimension()} dimensions\")\n",
    "print(f\"Model has subword information - can handle OOV words!\\n\")\n",
    "\n",
    "def vectorize_sentence(sentence_text: str, model, method: str = 'mean') -> Dict:\n",
    "    \"\"\"\n",
    "    Convert a sentence to a single vector by aggregating word vectors.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    sentence_text : str\n",
    "        The lemmatized sentence\n",
    "    model : fasttext.FastText._FastText\n",
    "        FastText model (loaded from .bin)\n",
    "    method : str\n",
    "        'mean' (average) or 'sum' (addition)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Dict with 'vector', 'words_found', 'words_total', 'coverage'\n",
    "    \n",
    "    References:\n",
    "    -----------\n",
    "    Arora et al. (2017) \"A Simple but Tough-to-Beat Baseline for Sentence Embeddings\"\n",
    "    \n",
    "    Note:\n",
    "    -----\n",
    "    FastText can generate vectors for most words using subword info, but very\n",
    "    unusual artifacts (like \"▪\" or \"│\") may still fail to produce meaningful vectors.\n",
    "    We check vector norms to identify such cases.\n",
    "    \"\"\"\n",
    "    words = sentence_text.split()\n",
    "    vectors = []\n",
    "    words_with_vectors = 0\n",
    "    \n",
    "    for word in words:\n",
    "        # FastText can get a vector for any word (uses subword info)\n",
    "        vector = model.get_word_vector(word)\n",
    "        \n",
    "        # Check if the vector is meaningful (not just zeros or near-zeros)\n",
    "        # Artifacts like \"▪\" might produce very low-norm vectors\n",
    "        if np.linalg.norm(vector) > 0.01:  # Threshold for meaningful vector\n",
    "            vectors.append(vector)\n",
    "            words_with_vectors += 1\n",
    "    \n",
    "    if len(vectors) == 0:\n",
    "        # No meaningful vectors - return zero vector\n",
    "        return {\n",
    "            'vector': np.zeros(model.get_dimension()),\n",
    "            'words_found': 0,\n",
    "            'words_total': len(words),\n",
    "            'coverage': 0.0\n",
    "        }\n",
    "    \n",
    "    vectors_array = np.array(vectors)\n",
    "    \n",
    "    # Aggregate vectors\n",
    "    if method == 'mean':\n",
    "        sentence_vector = np.mean(vectors_array, axis=0)\n",
    "    elif method == 'sum':\n",
    "        sentence_vector = np.sum(vectors_array, axis=0)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown aggregation method: {method}\")\n",
    "    \n",
    "    coverage = words_with_vectors / len(words) if len(words) > 0 else 0.0\n",
    "    \n",
    "    return {\n",
    "        'vector': sentence_vector,\n",
    "        'words_found': words_with_vectors,\n",
    "        'words_total': len(words),\n",
    "        'coverage': coverage\n",
    "    }\n",
    "\n",
    "print(f\"Vectorizing sentences using '{AGGREGATION_METHOD}' aggregation...\")\n",
    "print(\"This may take a few minutes...\\n\")\n",
    "\n",
    "vectorization_results = df_with_targets['sentence_text'].apply(\n",
    "    lambda x: vectorize_sentence(x, model, method=AGGREGATION_METHOD)\n",
    ")\n",
    "\n",
    "# Unpack results into separate columns\n",
    "df_with_targets['sentence_vector'] = vectorization_results.apply(lambda x: x['vector'])\n",
    "df_with_targets['words_found'] = vectorization_results.apply(lambda x: x['words_found'])\n",
    "df_with_targets['words_total'] = vectorization_results.apply(lambda x: x['words_total'])\n",
    "df_with_targets['coverage'] = vectorization_results.apply(lambda x: x['coverage'])\n",
    "\n",
    "df_vectorized = df_with_targets.copy()\n",
    "print(f\"Successfully vectorized {len(df_vectorized)} sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1240d2c2-8d20-4f7e-b8e3-b0e823898164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SUMMARY STATISTICS\n",
      "================================================================================\n",
      "\n",
      "Total vectorized sentences: 41311\n",
      "Average coverage: 99.93%\n",
      "Median coverage: 100.00%\n",
      "\n",
      "Coverage distribution:\n",
      "count    41311.000000\n",
      "mean         0.999326\n",
      "std          0.009487\n",
      "min          0.666667\n",
      "25%          1.000000\n",
      "50%          1.000000\n",
      "75%          1.000000\n",
      "max          1.000000\n",
      "Name: coverage, dtype: float64\n",
      "\n",
      "Sentences with coverage < 80%:\n",
      "  Count: 10 (0.02%)\n",
      "\n",
      "  Example low coverage sentences:\n",
      "    Coverage: 79.17% | Words: 38/48\n",
      "    Text: h spridning nukleär ämne stor brand avse bebyggd område skogsmark j) störning bränsle drivmedelsförs...\n",
      "\n",
      "    Coverage: 75.76% | Words: 25/33\n",
      "    Text: l) störning elektronisk kommunikation m) störning elförsörjning/ffjrvärmeförmeförmeförmeförmeförmefö...\n",
      "\n",
      "    Coverage: 73.17% | Words: 30/41\n",
      "    Text: avfallshantering 6) beredningskemikalium 1 dricksvatten 8) process teknisk vatten 3) elocent 12 fjär...\n",
      "\n",
      "\n",
      "Sentences by year:\n",
      "year\n",
      "2011           130\n",
      "2012            49\n",
      "2015          7759\n",
      "2016           650\n",
      "2017           120\n",
      "2019         11107\n",
      "2019.docx       39\n",
      "2020           673\n",
      "2021           134\n",
      "2023         19575\n",
      "2024           773\n",
      "Sala           302\n",
      "dtype: int64\n",
      "\n",
      "Sentences by municipality (top 10):\n",
      "municipality\n",
      "Fagersta        1411\n",
      "Östersund       983\n",
      "Norberg          938\n",
      "Örebro          933\n",
      "Ljusnarsberg     794\n",
      "Svedala          738\n",
      "Halmstad         725\n",
      "Kungälv         639\n",
      "Mora             611\n",
      "Stockholm        605\n",
      "dtype: int64\n",
      "\n",
      "================================================================================\n",
      "EXPANDING TO CATEGORY-SENTENCE PAIRS\n",
      "================================================================================\n",
      "Total category-sentence pairs: 47570\n",
      "\n",
      "Pairs by category:\n",
      "category\n",
      "resilience    10975\n",
      "risk          36595\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PART 4: SUMMARY STATISTICS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nTotal vectorized sentences: {len(df_vectorized)}\")\n",
    "print(f\"Average coverage: {df_vectorized['coverage'].mean():.2%}\")\n",
    "print(f\"Median coverage: {df_vectorized['coverage'].median():.2%}\")\n",
    "\n",
    "print(f\"\\nCoverage distribution:\")\n",
    "print(df_vectorized['coverage'].describe())\n",
    "\n",
    "print(f\"\\nSentences with coverage < 80%:\")\n",
    "low_coverage = df_vectorized[df_vectorized['coverage'] < 0.8]\n",
    "print(f\"  Count: {len(low_coverage)} ({len(low_coverage)/len(df_vectorized)*100:.2f}%)\")\n",
    "if len(low_coverage) > 0:\n",
    "    print(f\"\\n  Example low coverage sentences:\")\n",
    "    for idx in range(min(3, len(low_coverage))):\n",
    "        example = low_coverage.iloc[idx]\n",
    "        print(f\"    Coverage: {example['coverage']:.2%} | Words: {example['words_found']}/{example['words_total']}\")\n",
    "        print(f\"    Text: {example['sentence_text'][:100]}...\")\n",
    "        print()\n",
    "\n",
    "print(f\"\\nSentences by year:\")\n",
    "year_counts = df_vectorized.groupby('year').size().sort_index()\n",
    "print(year_counts)\n",
    "\n",
    "print(f\"\\nSentences by municipality (top 10):\")\n",
    "print(df_vectorized.groupby('municipality').size().sort_values(ascending=False).head(10))\n",
    "\n",
    "# Expand categories into separate rows for category-level analysis\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"EXPANDING TO CATEGORY-SENTENCE PAIRS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "rows_expanded = []\n",
    "for _, row in df_vectorized.iterrows():\n",
    "    for category in row['categories']:\n",
    "        rows_expanded.append({\n",
    "            'doc_id': row['doc_id'],\n",
    "            'municipality': row['municipality'],\n",
    "            'year': row['year'],\n",
    "            'maskad': row['maskad'],\n",
    "            'sentence_id': row['sentence_id'],\n",
    "            'category': category,\n",
    "            'target_terms': ', '.join(row['target_terms']),\n",
    "            'sentence_text': row['sentence_text'],\n",
    "            'sentence_vector': row['sentence_vector'],\n",
    "            'word_count': row['word_count'],\n",
    "            'words_found': row['words_found'],\n",
    "            'coverage': row['coverage']\n",
    "        })\n",
    "\n",
    "df_final = pd.DataFrame(rows_expanded)\n",
    "print(f\"Total category-sentence pairs: {len(df_final)}\")\n",
    "print(f\"\\nPairs by category:\")\n",
    "print(df_final.groupby('category').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e6754ca-2572-417b-8083-59a1f3b673cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SAVING RESULTS\n",
      "================================================================================\n",
      "Saved metadata to: sentence_vectors_metadata.csv\n",
      "Saved full data with vectors to: sentence_vectors_with_metadata.parquet\n",
      "Saved sentence vectors to: sentence_vectors.npy\n",
      "Shape: (47570, 300) (sentences × vector_dim)\n",
      "Saved index mapping to: sentence_vectors_index.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PART 5: SAVE RESULTS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAVING RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save metadata (without vectors for easy inspection)\n",
    "df_metadata = df_final.drop(columns=['sentence_vector'])\n",
    "df_metadata.to_csv('sentence_vectors_metadata.csv', index=False, encoding='utf-8')\n",
    "print(\"Saved metadata to: sentence_vectors_metadata.csv\")\n",
    "\n",
    "# Save full data with vectors to parquet (efficient for large data)\n",
    "df_final.to_parquet('sentence_vectors_with_metadata.parquet', index=False)\n",
    "print(\"Saved full data with vectors to: sentence_vectors_with_metadata.parquet\")\n",
    "\n",
    "# Save just the vectors as numpy array (for quick loading in analysis)\n",
    "sentence_vectors = np.stack(df_final['sentence_vector'].values)\n",
    "np.save('sentence_vectors.npy', sentence_vectors)\n",
    "print(f\"Saved sentence vectors to: sentence_vectors.npy\")\n",
    "print(f\"Shape: {sentence_vectors.shape} (sentences × vector_dim)\")\n",
    "\n",
    "# Save index mapping (to link numpy array back to metadata)\n",
    "df_final.reset_index(drop=True).to_csv(\n",
    "    'sentence_vectors_index.csv',\n",
    "    columns=['doc_id', 'municipality', 'year', 'category'],\n",
    "    index=True,\n",
    "    encoding='utf-8'\n",
    ")\n",
    "print(\"Saved index mapping to: sentence_vectors_index.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1225a937-4176-4f2b-b493-4c34e221dc63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EXAMPLE: AVERAGE VECTORS BY YEAR AND CATEGORY\n",
      "================================================================================\n",
      "\n",
      "RESILIENCE:\n",
      "  2011:   42 sentences, centroid norm:  0.398, avg coverage: 99.69%\n",
      "  2012:   14 sentences, centroid norm:  0.426, avg coverage: 100.00%\n",
      "  2015: 1966 sentences, centroid norm:  0.410, avg coverage: 99.96%\n",
      "  2016:  200 sentences, centroid norm:  0.416, avg coverage: 100.00%\n",
      "  2017:   20 sentences, centroid norm:  0.404, avg coverage: 100.00%\n",
      "  2019: 2802 sentences, centroid norm:  0.408, avg coverage: 99.93%\n",
      "  2019.docx:   18 sentences, centroid norm:  0.431, avg coverage: 100.00%\n",
      "  2020:  205 sentences, centroid norm:  0.423, avg coverage: 100.00%\n",
      "  2021:   52 sentences, centroid norm:  0.389, avg coverage: 100.00%\n",
      "  2023: 5346 sentences, centroid norm:  0.403, avg coverage: 99.97%\n",
      "  2024:  251 sentences, centroid norm:  0.401, avg coverage: 99.95%\n",
      "  Sala:   59 sentences, centroid norm:  0.426, avg coverage: 100.00%\n",
      "\n",
      "RISK:\n",
      "  2011:  111 sentences, centroid norm:  0.445, avg coverage: 99.84%\n",
      "  2012:   46 sentences, centroid norm:  0.366, avg coverage: 100.00%\n",
      "  2015: 6912 sentences, centroid norm:  0.447, avg coverage: 99.94%\n",
      "  2016:  576 sentences, centroid norm:  0.441, avg coverage: 100.00%\n",
      "  2017:  107 sentences, centroid norm:  0.476, avg coverage: 99.97%\n",
      "  2019: 9972 sentences, centroid norm:  0.447, avg coverage: 99.96%\n",
      "  2019.docx:   38 sentences, centroid norm:  0.408, avg coverage: 100.00%\n",
      "  2020:  599 sentences, centroid norm:  0.449, avg coverage: 99.75%\n",
      "  2021:  106 sentences, centroid norm:  0.447, avg coverage: 100.00%\n",
      "  2023: 17211 sentences, centroid norm:  0.441, avg coverage: 99.92%\n",
      "  2024:  633 sentences, centroid norm:  0.438, avg coverage: 99.94%\n",
      "  Sala:  284 sentences, centroid norm:  0.446, avg coverage: 100.00%\n",
      "\n",
      "================================================================================\n",
      "EXAMPLE: COMPARING YEARS WITHIN CATEGORY\n",
      "================================================================================\n",
      "\n",
      "RESILIENCE - Year-to-Year Similarity:\n",
      "  2011 → 2012: 0.9584\n",
      "  2012 → 2015: 0.9805\n",
      "  2015 → 2016: 0.9966\n",
      "  2016 → 2017: 0.9505\n",
      "  2017 → 2019: 0.9579\n",
      "  2019 → 2019.docx: 0.9636\n",
      "  2019.docx → 2020: 0.9546\n",
      "  2020 → 2021: 0.9824\n",
      "  2021 → 2023: 0.9895\n",
      "  2023 → 2024: 0.9948\n",
      "  2024 → Sala: 0.9876\n",
      "\n",
      "RISK - Year-to-Year Similarity:\n",
      "  2011 → 2012: 0.9578\n",
      "  2012 → 2015: 0.9561\n",
      "  2015 → 2016: 0.9943\n",
      "  2016 → 2017: 0.9794\n",
      "  2017 → 2019: 0.9847\n",
      "  2019 → 2019.docx: 0.9805\n",
      "  2019.docx → 2020: 0.9757\n",
      "  2020 → 2021: 0.9874\n",
      "  2021 → 2023: 0.9845\n",
      "  2023 → 2024: 0.9974\n",
      "  2024 → Sala: 0.9863\n",
      "\n",
      "================================================================================\n",
      "DATA READY FOR ANALYSIS!\n",
      "================================================================================\n",
      "\n",
      "You now have:\n",
      "1. sentence_vectors_with_metadata.parquet - Full dataset with vectors\n",
      "2. sentence_vectors_metadata.csv - Metadata for inspection\n",
      "3. sentence_vectors.npy - Just the vectors for efficient loading\n",
      "4. sentence_vectors_index.csv - Index to map vectors to metadata\n",
      "\n",
      "Next steps:\n",
      "- Temporal analysis: Track how contexts change over time\n",
      "- Municipality comparison: Compare semantic spaces across regions\n",
      "- Clustering: Group similar contexts\n",
      "- Dimensionality reduction: Visualize with PCA/t-SNE\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PART 6: EXAMPLE ANALYSES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXAMPLE: AVERAGE VECTORS BY YEAR AND CATEGORY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for category in sorted(df_final['category'].unique()):\n",
    "    print(f\"\\n{category.upper()}:\")\n",
    "    cat_data = df_final[df_final['category'] == category]\n",
    "    \n",
    "    for year in sorted(cat_data['year'].unique()):\n",
    "        year_data = cat_data[cat_data['year'] == year]\n",
    "        year_vectors = np.stack(year_data['sentence_vector'].values)\n",
    "        centroid = np.mean(year_vectors, axis=0)\n",
    "        \n",
    "        print(f\"  {year}: {len(year_data):4d} sentences, \"\n",
    "              f\"centroid norm: {np.linalg.norm(centroid):6.3f}, \"\n",
    "              f\"avg coverage: {year_data['coverage'].mean():.2%}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXAMPLE: COMPARING YEARS WITHIN CATEGORY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Example: Calculate cosine similarity between year centroids\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "for category in sorted(df_final['category'].unique()):\n",
    "    cat_data = df_final[df_final['category'] == category]\n",
    "    years = sorted(cat_data['year'].unique())\n",
    "    \n",
    "    if len(years) < 2:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n{category.upper()} - Year-to-Year Similarity:\")\n",
    "    \n",
    "    # Calculate centroid for each year\n",
    "    year_centroids = {}\n",
    "    for year in years:\n",
    "        year_data = cat_data[cat_data['year'] == year]\n",
    "        year_vectors = np.stack(year_data['sentence_vector'].values)\n",
    "        year_centroids[year] = np.mean(year_vectors, axis=0)\n",
    "    \n",
    "    # Calculate similarities between consecutive years\n",
    "    for i in range(len(years) - 1):\n",
    "        year1, year2 = years[i], years[i+1]\n",
    "        sim = cosine_similarity(\n",
    "            year_centroids[year1].reshape(1, -1),\n",
    "            year_centroids[year2].reshape(1, -1)\n",
    "        )[0, 0]\n",
    "        print(f\"  {year1} → {year2}: {sim:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA READY FOR ANALYSIS!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nYou now have:\")\n",
    "print(\"1. sentence_vectors_with_metadata.parquet - Full dataset with vectors\")\n",
    "print(\"2. sentence_vectors_metadata.csv - Metadata for inspection\")\n",
    "print(\"3. sentence_vectors.npy - Just the vectors for efficient loading\")\n",
    "print(\"4. sentence_vectors_index.csv - Index to map vectors to metadata\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"- Temporal analysis: Track how contexts change over time\")\n",
    "print(\"- Municipality comparison: Compare semantic spaces across regions\")\n",
    "print(\"- Clustering: Group similar contexts\")\n",
    "print(\"- Dimensionality reduction: Visualize with PCA/t-SNE\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
